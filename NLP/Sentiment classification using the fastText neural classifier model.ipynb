{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP- task4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQUUMh-D6VG9"
      },
      "source": [
        "# Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fixCtMZU0_H"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "MAX_INPUT_SIZE = 200\n",
        "\n",
        "!pip install bpemb\n",
        "from bpemb import BPEmb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KXF-0Fe6dih"
      },
      "source": [
        "# Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IzxQWVh5fTn"
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhoxsVBMUzjx"
      },
      "source": [
        "!tar xvzf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL4juvozU08n"
      },
      "source": [
        "train_data = []\n",
        "test_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeEq6hWvVGgI"
      },
      "source": [
        "# Getting the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Itld5kdX6kAY"
      },
      "source": [
        "The data should be extracted from the folders and concatenated into train and test sets. Aditionally, in this part, each data input is tokenized and assigned to a one-hot ecoded label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfODUo9lU05v"
      },
      "source": [
        "directory = 'aclImdb/train/neg/'\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = open(directory + '/' + filename, 'r')\n",
        "    lines = f.read()\n",
        "    doc = nlp(lines)\n",
        "    train_data.append([doc, [0,1]])\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbZY8XwSU02f"
      },
      "source": [
        "directory = 'aclImdb/train/pos/'\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = open(directory + '/' + filename, 'r')\n",
        "    lines = f.read()\n",
        "    doc = nlp(lines)\n",
        "    train_data.append([doc, [1,0]])\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbfCqupeU0zn"
      },
      "source": [
        "directory = 'aclImdb/test/neg/'\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = open(directory + '/' + filename, 'r')\n",
        "    lines = f.read()\n",
        "    doc = nlp(lines)\n",
        "    test_data.append([doc, [0,1]])\n",
        "    f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjFHg1dOU_vZ"
      },
      "source": [
        "directory = 'aclImdb/test/pos/'\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = open(directory + '/' + filename, 'r')\n",
        "    lines = f.read()\n",
        "    doc = nlp(lines)\n",
        "    test_data.append([doc,[1,0]])\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE9PxUB8VYMo"
      },
      "source": [
        "# Create a mapping from words to IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNWigUzX7Pyn"
      },
      "source": [
        "Each token shuold have a specific ID that represents it, so we can transform the sentenses to ID vectors for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbnprhgpVhAA"
      },
      "source": [
        "reserve_test = test_data\n",
        "reserve_train = train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb9ROLlVU_sW"
      },
      "source": [
        "all_data = train_data + test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB_leyXyU_pp"
      },
      "source": [
        "def assign_ids(doc_list):\n",
        "    words_list = []\n",
        "    for row in doc_list:\n",
        "        for token in row[0]:\n",
        "            words_list.append(token.text.lower())\n",
        "    words_list = list(dict.fromkeys(words_list))\n",
        "    return {k: v+1 for v, k in enumerate(words_list)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQINqF2WVnqM"
      },
      "source": [
        "mapping = assign_ids(all_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYWvleyKVnn-"
      },
      "source": [
        "NUM_WORDS = len(mapping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV0sFqGIVvEe"
      },
      "source": [
        "# Transform sentences to ID lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHb9Y5tDVnlJ"
      },
      "source": [
        "def string_to_model_input(sentence): \n",
        " \n",
        "  X=[]\n",
        "  for token in sentence:\n",
        "    X.append(mapping[token.text.lower()])\n",
        "  #padding\n",
        "  aux = len(X)\n",
        "  X = ([0] * MAX_INPUT_SIZE + X)[aux:]\n",
        " \n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSyAQxvQVnin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70203128-08d0-4685-d38c-44c5bc6c0cb9"
      },
      "source": [
        "train_data = np.array(train_data)\n",
        "test_data = np.array(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDudqTzVV7on"
      },
      "source": [
        "Y_train = train_data[:, -1]\n",
        "Y_test = test_data[:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q70eVESTV9a2"
      },
      "source": [
        "X_train = [string_to_model_input(line[0]) for line in train_data ]\n",
        "X_test = [string_to_model_input(line[0]) for line in test_data ]\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdI9hIw1WD44",
        "outputId": "2d2e330c-9987-40da-c3e3-0c45db4b0150"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 200)\n",
            "(25000, 200)\n",
            "(25000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K3qKRYdWG_X"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP2CUVI0JCct"
      },
      "source": [
        "constructing and training the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtVZoUvVWJXR"
      },
      "source": [
        "def build_simple_model():\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Embedding(NUM_WORDS+1, 100 , input_length=MAX_INPUT_SIZE )) \n",
        "  model.add(keras.layers.GlobalMaxPooling1D())\n",
        "  model.add(keras.layers.Dense(2, activation= \"softmax\" ))\n",
        "\n",
        "  model. compile (loss=keras.losses.categorical_crossentropy, optimizer='Adam', metrics = [ 'accuracy' ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic-NY0QgWL-9"
      },
      "source": [
        "model = build_simple_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQsKYGGHWhyO",
        "outputId": "6681fbf9-6993-4d37-d03d-6d9a6b5c917f"
      },
      "source": [
        "X = X_train.tolist()\n",
        "Y = Y_train.tolist()\n",
        "Y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzaLpH6LWjal"
      },
      "source": [
        "model.fit(X, Y, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwnGNC9EWvWF"
      },
      "source": [
        "X_t = X_test.tolist()\n",
        "Y_t = Y_test.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAYee9D-W08o"
      },
      "source": [
        "Y_pred = model.predict([X_test])\n",
        "Y_pred = np.round(Y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh4sHkj2W6e8",
        "outputId": "fcfdef3f-daa0-453d-c866-96ca988d6a2b"
      },
      "source": [
        "accuracy_score(Y_t, Y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8612"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkyxaViPXNbG"
      },
      "source": [
        "# Model 2 (fastText)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BfcxEajOaN5"
      },
      "source": [
        "## get the pretrained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFBO2nQzN2PL"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxPf6HhAyHea"
      },
      "source": [
        "!unzip wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbVIlRIOrYIT"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "FASTTEXTFILE = \"wiki-news-300d-1M.vec\"\n",
        "ft_model = KeyedVectors.load_word2vec_format(FASTTEXTFILE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwhbkjeatAOR"
      },
      "source": [
        "embedding_matrix =  np.zeros((NUM_WORDS, 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpD8Rmb1yI9C"
      },
      "source": [
        "hits = 0\n",
        "misses = 0\n",
        "i = 0\n",
        "for token in mapping:\n",
        "  try:\n",
        "    embedding_vector = ft_model.get_vector(str(token))\n",
        "    embedding_matrix[i] = embedding_vector[:100]\n",
        "    hits += 1\n",
        "  except Exception as e:\n",
        "    if \"not in vocabulary\" in str(e):\n",
        "      misses += 1\n",
        "  i+=1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikN1wUGyOlO0"
      },
      "source": [
        "## construct the model with initialized weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwzDtNAByQ1W"
      },
      "source": [
        "def build_ft_model():\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Embedding(NUM_WORDS, 100 , input_length=MAX_INPUT_SIZE, weights=[embedding_matrix], trainable=False )) \n",
        "  model.add(keras.layers.GlobalMaxPooling1D())\n",
        "  model.add(keras.layers.Dense(2, activation= \"softmax\" ))\n",
        "\n",
        "  model. compile (loss=keras.losses.categorical_crossentropy, optimizer='Adam', metrics = [ 'accuracy' ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfwokSqb6j_P"
      },
      "source": [
        "model = build_ft_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM3NJUa56q4X"
      },
      "source": [
        "model.fit(X, Y, epochs = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9uZH-yB6zA3"
      },
      "source": [
        "Y_pred = model.predict([X_test])\n",
        "Y_pred = np.round(Y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6UtJ7_v65_0",
        "outputId": "4822de24-d69f-4b60-bbf3-0a167f674304"
      },
      "source": [
        "accuracy_score(Y_t, Y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5584"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R2es3wiOsqe"
      },
      "source": [
        "# model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-_2gL0n66WT"
      },
      "source": [
        "bpemb_en = BPEmb(lang=\"en\", dim=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_fx4bgVhQQR"
      },
      "source": [
        "train_data[0][0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNYditL2cHVg"
      },
      "source": [
        "encode_ids_train = bpemb_en.encode_ids(train_data[0][0].text)\n",
        "encodes = bpemb_en.encode(train_data[0][0].text)\n",
        "encode_ids_test = bpemb_en.encode_ids(train_data[0][0].text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMn9woU_gemU"
      },
      "source": [
        "X_bpm_train = []\n",
        "for line in train_data:\n",
        "  X_bpm_train.append(bpemb_en.encode_ids(line[0].text)[:MAX_INPUT_SIZE])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_3ebYBHpJEW"
      },
      "source": [
        "len(X_bpm_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdqVT8ENhria"
      },
      "source": [
        "X_bpm_test = []\n",
        "for line in test_data:\n",
        "  X_bpm_test.append(bpemb_en.encode_ids(line[0].text)[:MAX_INPUT_SIZE])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkNRpZ6XpqJY"
      },
      "source": [
        "all_bpm =  X_bpm_train + X_bpm_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJI45fR2lraL"
      },
      "source": [
        "all_ids=[]\n",
        "for row in all_bpm:\n",
        "  all_ids += row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJFBQkhrrG0r"
      },
      "source": [
        "len(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apPRnJq8rFzS"
      },
      "source": [
        "all_ids = list(dict.fromkeys(all_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SCMQJqElGP2"
      },
      "source": [
        "embedding_matrix =  np.zeros((NUM_WORDS, 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9z7nEprm74x"
      },
      "source": [
        "hits = 0\n",
        "misses = 0\n",
        "i = 0\n",
        "for id in all_ids:\n",
        "  try:\n",
        "    embedding_vector = bpemb_en.vectors[id]\n",
        "    embedding_matrix[i] = embedding_vector[:100]\n",
        "    hits += 1\n",
        "  except Exception as e:\n",
        "    if \"not in vocabulary\" in str(e):\n",
        "      misses += 1\n",
        "  i+=1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn8h2ZgXrXV4"
      },
      "source": [
        "def build_bpm_model():\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Embedding(NUM_WORDS, 100 , input_length=MAX_INPUT_SIZE, weights=[embedding_matrix], trainable=False )) \n",
        "  model.add(keras.layers.GlobalMaxPooling1D())\n",
        "  model.add(keras.layers.Dense(2, activation= \"softmax\" ))\n",
        "\n",
        "  model. compile (loss=keras.losses.categorical_crossentropy, optimizer='Adam', metrics = [ 'accuracy' ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZDu0SbSsCLF"
      },
      "source": [
        "model = build_bpm_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0zngAH3s44n"
      },
      "source": [
        "X_bpm_train_resized = []\n",
        "for sent in X_bpm_train:\n",
        "  aux = len(sent)\n",
        "  sent = ([0] * MAX_INPUT_SIZE + sent)[aux:]\n",
        "  X_bpm_train_resized.append(sent)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JHFekATxO7_"
      },
      "source": [
        "X_bpm_test_resized = []\n",
        "for sent in X_bpm_test:\n",
        "  aux = len(sent)\n",
        "  sent = ([0] * MAX_INPUT_SIZE + sent)[aux:]\n",
        "  X_bpm_test_resized.append(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQCMNy7VtzPM",
        "outputId": "4d5686e4-f538-47c8-dd66-64abfcf03e36"
      },
      "source": [
        "len(X_bpm_test_resized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaFr3p5csHM-"
      },
      "source": [
        "model.fit(X_bpm_train_resized, Y, epochs = 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AgwJjzksaUA"
      },
      "source": [
        "Y_pred = model.predict(X_bpm_test_resized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exw8jWQKxi_-"
      },
      "source": [
        "Y_pred = np.round(Y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayb2m_BttC0B",
        "outputId": "7b804705-4f41-4ff7-b8d1-b2b318caca53"
      },
      "source": [
        "accuracy_score(Y_t, Y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    }
  ]
}