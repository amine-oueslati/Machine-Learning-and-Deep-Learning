{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oueslati Amine (W9GDX2)-Assignment#2-NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_iuM9s1NrzI"
      },
      "source": [
        "# Oueslati Amine (W9GDX2)-Assignment#2-NLP\n",
        "\n",
        "### Importing the necessary libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM9iWqFfgp6A",
        "outputId": "da91da18-42e9-44dd-f50a-ec7b742e78c9"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "!pip install sklearn-crfsuite\n",
        "\n",
        "import sklearn\n",
        "import sklearn_crfsuite\n",
        "\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import ast\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4YZdivsOe6N"
      },
      "source": [
        "### Getting tagged sentences with the universal tagset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW6qDbcYepjh"
      },
      "source": [
        "data = brown.tagged_sents(tagset='universal')\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChEWxEDLPNVN"
      },
      "source": [
        "---\n",
        "\n",
        "The zip function is used here to get the first element of each list in a sentence. \n",
        "\n",
        "Thus, the result will be a list of lists of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5UYw59DaHUO"
      },
      "source": [
        "words=[list(list(zip(*sent))[0]) for sent in data]\n",
        "words[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZTfq292RgwK"
      },
      "source": [
        "The same methode is used here to get the list of labels, which is a list of lists of tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_umlZtSwqAj6"
      },
      "source": [
        "labels=[list(list(zip(*sent))[1]) for sent in data]\n",
        "labels[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2nZdBsFR7y_"
      },
      "source": [
        "---\n",
        "### Feature extraction\n",
        "This function is responsible for the transformation of a list of tokens to a list of dictionaries that contains the required features for the CRF model.\n",
        "\n",
        "SpaCy is used for the feature extraction, therefore, the input list should be transformed to a spaCy document. \n",
        "\n",
        "knowing that the CRF algorithm needs the features of the previous, the current, and the next word to get better accuracy and efficiency; each dictionary will contain 3 sets of the same features (except for the first and the last words).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbaDQCK8e8Cb"
      },
      "source": [
        "def token2features(list):\n",
        "    doc = Doc(nlp.vocab, words=list)\n",
        "    listOfFeatures=[]\n",
        "    \n",
        "    for i  in range(len(doc)):\n",
        "      features = {\n",
        "          'lower': doc[i].lower_,\n",
        "          'suffix': doc[i].suffix_,\n",
        "          'prefix': doc[i].prefix_,\n",
        "          'isupper': doc[i].is_upper,\n",
        "          'istitle': doc[i].is_title,\n",
        "          'isdigit': doc[i].is_digit\n",
        "      }\n",
        "      if i > 0:\n",
        "          features.update({\n",
        "              '-1_lower': doc[i-1].lower_,\n",
        "              '-1_suffix': doc[i-1].suffix_,\n",
        "              '-1_prefix': doc[i-1].prefix_,\n",
        "              '-1_isupper': doc[i-1].is_upper,\n",
        "              '-1_istitle': doc[i-1].is_title,\n",
        "              '-1_isdigit': doc[i-1].is_digit\n",
        "          })\n",
        "      else:\n",
        "          features['BOS'] = True\n",
        "\n",
        "      if i < len(doc)-1:\n",
        "          features.update({\n",
        "              '+1_lower': doc[i+1].lower_,\n",
        "              '+1_suffix': doc[i+1].suffix_,\n",
        "              '+1_prefix': doc[i+1].prefix_,\n",
        "              '+1_isupper': doc[i+1].is_upper,\n",
        "              '+1_istitle': doc[i+1].is_title,\n",
        "              '+1_isdigit': doc[i+1].is_digit,\n",
        "          })\n",
        "      else:\n",
        "          features['EOS'] = True\n",
        "      \n",
        "      listOfFeatures.append(features)\n",
        "\n",
        "    return listOfFeatures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuTu0vVTVLJs"
      },
      "source": [
        "Since the **token2features** function works for just a list of tokens ( a sentence ) , an iterative methode is required for preprocessing the whole data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lswSAMZN3geA"
      },
      "source": [
        "listOFListsOfDictionaries  = [token2features(sent) for sent in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtOc3EUo6lkQ"
      },
      "source": [
        "listOFListsOfDictionaries[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROVGpJeKWHSu"
      },
      "source": [
        "### Splitting the data\n",
        "\n",
        "The labels are already stored in the **labels** list and the text is preprocessed, so, the data can be splitted to a training set and a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBj6Qx6v6sib"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(listOFListsOfDictionaries, labels, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD3aasurW3Ff"
      },
      "source": [
        "### Traning the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2rDzCfU-jFY"
      },
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku_Glu6sXHxA"
      },
      "source": [
        "To evaluate the trained model, the test data set is used to predict the labels and compare it with the true ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0_y4Sq1_Xu-",
        "outputId": "f8ddaa2b-34f6-4a39-c776-9cea36645bd2"
      },
      "source": [
        "y_pred = crf.predict(X_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9784649763630752"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zuP1hM4ZbjC"
      },
      "source": [
        "### Pos_tagger function\n",
        "This function accepts a string or a list of tokens and output the pos_tags.\n",
        "\n",
        "The type of the input is verified, so if the input is a string it will be transformed to a list of tokens. Then the CRF model will predict the tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmPE8-EfAoxL"
      },
      "source": [
        "def pos_tagger(sent):\n",
        "  if type(sent) == str:\n",
        "    doc = nlp(sent)\n",
        "    for token in doc:\n",
        "      tokens  = [token.text for token in doc]\n",
        "  elif type(sent) == list:\n",
        "    tokens = sent\n",
        "  data = [token2features(tokens)]\n",
        "\n",
        "  result = crf.predict(data)\n",
        "  return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRu-4m0qDKK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba64222-cdec-46cb-d31c-be46bc78f93c"
      },
      "source": [
        "sent = input()\n",
        "print(\"\\n\", pos_tagger(sent)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            "\n",
            " ['NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFPMkJ5vgvd3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}